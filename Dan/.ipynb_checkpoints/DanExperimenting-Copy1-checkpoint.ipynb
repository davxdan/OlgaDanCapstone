{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from scipy.fftpack import fft, rfft\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import moment\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "import scipy.stats as st\n",
    "from scipy.stats import randint as sp_randint\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "sns.set()\n",
    "path=\"C:/Users/danie/Documents/GitHub/OlgaDanCapstone/GPUProject\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a9b3a034dc333abb2c7c94d00300354b0c6fdc61"
   },
   "outputs": [],
   "source": [
    "# test_file = pd.read_csv('seg_00a37e.csv')\n",
    "# print(\"Segment shape\", test_file.shape)\n",
    "# test_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "7abc992d4c03d57c543d6fc41fefc9e008d401dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\n",
    "# test_file = pd.read_csv('seg_00a37e.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db0ef041c84fcb1f60fdea9450d3fa8476cf4b58"
   },
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.precision\", 15)  # show more decimals\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d34d5f3273f5dc161aa8207fbb503f14684728c"
   },
   "source": [
    "There are outliers in both directions; let's try to plot the same distribution with x in the range -20 to 30. The black line is the closest normal distribution (gaussian) possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: We will divide the train data set for segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the training data in chunks of 150,000 examples and extract the following features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 77175/150010 [20:57<19:52, 61.06it/s]"
     ]
    }
   ],
   "source": [
    "from scipy.fftpack import fft, rfft\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import moment\n",
    "rows = 4194\n",
    "segments = int(np.floor(train.shape[0] / rows))\n",
    "#segments = 150000 \n",
    "X_train = pd.DataFrame(index=range(segments), dtype=np.float64)\n",
    "y_train = pd.DataFrame(index=range(segments), dtype=np.float64)\n",
    "\n",
    "for segment in tqdm(range(segments)):\n",
    "    seg = train.iloc[segment*rows:segment*rows+rows]\n",
    "    x = seg['acoustic_data']   # pd series\n",
    "    y = seg['time_to_failure'].values[-1]  # single value\n",
    "\n",
    "    y_train.loc[segment, 'time_to_failure'] = np.sqrt(y)\n",
    "    \n",
    "    I_ave10 = 0.1*len(x)\n",
    "    II_ave10 = 0.2*len(x)\n",
    "    III_ave10 = 0.3*len(x)\n",
    "    IV_ave10 = 0.4*len(x)\n",
    "    V_ave10 = 0.5*len(x)\n",
    "    VI_ave10 = 0.6*len(x)\n",
    "    VII_ave10 = 0.7*len(x)\n",
    "    VIII_ave10 = 0.8*len(x)\n",
    "    IX_ave10 = 0.9*len(x)\n",
    "\n",
    "    X_train.loc[segment, '5<x<10'] = sum(x.between(5, 10))    \n",
    "    X_train.loc[segment, '10<x<20'] = sum(x.between(10, 20))\n",
    "    X_train.loc[segment, '20<x<30'] = sum(x.between(20, 30))\n",
    "    X_train.loc[segment, '30<x<100'] = sum(x.between(30, 100))\n",
    "    X_train.loc[segment, '100<x<500'] = sum(x.between(100, 500))\n",
    "    X_train.loc[segment, '500<x<1000'] = sum(x.between(500, 1000))\n",
    "    X_train.loc[segment, 'ave'] = x.values.mean()\n",
    "    X_train.loc[segment, 'std'] = x.values.std()\n",
    "    X_train.loc[segment, 'max'] = x.values.max()\n",
    "    X_train.loc[segment, 'min'] = x.values.min()\n",
    "        \n",
    "    X_train.loc[segment, 'q90'] = np.quantile(x.values, 0.90)\n",
    "    X_train.loc[segment, 'q95'] = np.quantile(x.values, 0.95)\n",
    "    X_train.loc[segment, 'q99'] = np.quantile(x.values, 0.99)\n",
    "    X_train.loc[segment, 'q05'] = np.quantile(x.values, 0.05)\n",
    "    X_train.loc[segment, 'q10'] = np.quantile(x.values, 0.10)\n",
    "    X_train.loc[segment, 'q01'] = np.quantile(x.values, 0.01)\n",
    "    \n",
    "    X_train.loc[segment, 'abs_max'] = np.abs(x.values).max()\n",
    "    X_train.loc[segment, 'abs_mean'] = np.abs(x.values).mean()\n",
    "    X_train.loc[segment, 'abs_std'] = np.abs(x.values).std()\n",
    "        \n",
    "     # New features - rolling features\n",
    "    for w in [100, 1000]:\n",
    "        x_roll_std = x.rolling(w).std().dropna().values\n",
    "        x_roll_mean = x.rolling(w).mean().dropna().values\n",
    "        x_roll_abs_mean = x.abs().rolling(w).mean().dropna().values\n",
    "        X_train.loc[segment, 'ave_roll_std_' + str(w)] = x_roll_std.mean()\n",
    "        X_train.loc[segment, 'std_roll_std_' + str(w)] = x_roll_std.std()\n",
    "        X_train.loc[segment, 'max_roll_std_' + str(w)] = x_roll_std.max()\n",
    "        X_train.loc[segment, 'min_roll_std_' + str(w)] = x_roll_std.min()\n",
    "        X_train.loc[segment, 'ave_roll_mean_' + str(w)] = x_roll_mean.mean()\n",
    "        X_train.loc[segment, 'std_roll_mean_' + str(w)] = x_roll_mean.std()\n",
    "        X_train.loc[segment, 'max_roll_mean_' + str(w)] = x_roll_mean.max()\n",
    "        X_train.loc[segment, 'min_roll_mean_' + str(w)] = x_roll_mean.min()\n",
    "        X_train.loc[segment, 'q01_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.01)\n",
    "        X_train.loc[segment, 'q05_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.05)\n",
    "        X_train.loc[segment, 'q10_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.10)\n",
    "        X_train.loc[segment, 'q95_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.95)\n",
    "        X_train.loc[segment, 'q99_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.99)\n",
    "        X_train.loc[segment, 'q01_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.01)\n",
    "        X_train.loc[segment, 'q05_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Amplitude Spectral Density ASDs are the square root of the power spectral densities (PSDs), \n",
    "#which are averages of the square of the fast fourier transforms (FFTs) of the data.\n",
    "\n",
    "# from scipy.fftpack import fft, rfft\n",
    "# import math\n",
    "# from tqdm import tqdm\n",
    "# from scipy.stats import moment\n",
    "# rows = 150_000\n",
    "# segments = int(np.floor(train.shape[0] / rows))\n",
    "\n",
    "# X_train = pd.DataFrame(index=range(segments), dtype=np.float64)\n",
    "# y_train = pd.DataFrame(index=range(segments), dtype=np.float64)\n",
    "\n",
    "# for segment in tqdm(range(segments)):\n",
    "#     seg = train.iloc[segment*rows:segment*rows+rows]\n",
    "#     #seg = seg[seg.acoustic_data.between(-20,30)]\n",
    "#     x = seg['acoustic_data']   # pd series\n",
    "#     y = seg['time_to_failure'].values[-1]  # single value\n",
    "    \n",
    "#     #y_train.loc[segment, 'time_to_failure'] = np.log(y)\n",
    "#     y_train.loc[segment, 'time_to_failure'] = np.sqrt(y)\n",
    "    \n",
    "#     I_ave10 = 0.1*len(x)\n",
    "#     II_ave10 = 0.2*len(x)\n",
    "#     III_ave10 = 0.3*len(x)\n",
    "#     IV_ave10 = 0.4*len(x)\n",
    "#     V_ave10 = 0.5*len(x)\n",
    "#     VI_ave10 = 0.6*len(x)\n",
    "#     VII_ave10 = 0.7*len(x)\n",
    "#     VIII_ave10 = 0.8*len(x)\n",
    "#     IX_ave10 = 0.9*len(x)\n",
    "    \n",
    "    \n",
    "#     X_train.loc[segment, 'rFFT mean'] = rfft(x).mean()\n",
    "#     X_train.loc[segment, 'rFFT std'] = rfft(x).std()\n",
    "       \n",
    "#     X_train.loc[segment, '0<x<5'] = sum(x.between(0, 5))\n",
    "#     X_train.loc[segment, '5<x<10'] = sum(x.between(5, 10))    \n",
    "#     X_train.loc[segment, '10<x<20'] = sum(x.between(10, 20))\n",
    "#     X_train.loc[segment, '20<x<30'] = sum(x.between(20, 30))\n",
    "#     X_train.loc[segment, '30<x<100'] = sum(x.between(30, 100))\n",
    "#     X_train.loc[segment, '100<x<500'] = sum(x.between(100, 500))\n",
    "#     X_train.loc[segment, '500<x<1000'] = sum(x.between(500, 1000))\n",
    "#     X_train.loc[segment, '1000<x<5000'] = sum(x.between(1000, 5000))\n",
    "    \n",
    "#     X_train.loc[segment, 'skewness'] = moment(x, moment=3)\n",
    "#     X_train.loc[segment, 'kurtosis'] = moment(x, moment=4)\n",
    "#     X_train.loc[segment, 'hypermoment'] = moment(x, moment=5)\n",
    "#     X_train.loc[segment, 'hypermoment2'] = moment(x, moment=6)\n",
    "#     X_train.loc[segment, 'ave'] = x.values.mean()\n",
    "#     X_train.loc[segment, 'std'] = x.values.std()\n",
    "#     X_train.loc[segment, 'max'] = x.values.max()\n",
    "#     X_train.loc[segment, 'min'] = x.values.min()\n",
    "        \n",
    "#     X_train.loc[segment, 'q90'] = np.quantile(x.values, 0.90)\n",
    "#     X_train.loc[segment, 'q95'] = np.quantile(x.values, 0.95)\n",
    "#     X_train.loc[segment, 'q99'] = np.quantile(x.values, 0.99)\n",
    "#     X_train.loc[segment, 'q05'] = np.quantile(x.values, 0.05)\n",
    "#     X_train.loc[segment, 'q10'] = np.quantile(x.values, 0.10)\n",
    "#     X_train.loc[segment, 'q01'] = np.quantile(x.values, 0.01)\n",
    "    \n",
    "#     X_train.loc[segment, 'abs_max'] = np.abs(x.values).max()\n",
    "#     X_train.loc[segment, 'abs_mean'] = np.abs(x.values).mean()\n",
    "#     X_train.loc[segment, 'abs_std'] = np.abs(x.values).std()\n",
    "        \n",
    "#      # New features - rolling features\n",
    "#     for w in [100, 1000]:\n",
    "#         x_roll_std = x.rolling(w).std().dropna().values\n",
    "#         x_roll_mean = x.rolling(w).mean().dropna().values\n",
    "#         x_roll_abs_mean = x.abs().rolling(w).mean().dropna().values\n",
    "        \n",
    "#         X_train.loc[segment, 'ave_roll_std_' + str(w)] = x_roll_std.mean()\n",
    "#         X_train.loc[segment, 'std_roll_std_' + str(w)] = x_roll_std.std()\n",
    "#         X_train.loc[segment, 'max_roll_std_' + str(w)] = x_roll_std.max()\n",
    "#         X_train.loc[segment, 'min_roll_std_' + str(w)] = x_roll_std.min()\n",
    "#         #X_train.loc[segment, 'mean_rFFT_roll_std_' + str(w)] = rfft(x_roll_std).mean()\n",
    "#         #X_train.loc[segment, 'std_rFFT_roll_std_' + str(w)] = rfft(x_roll_std).std()\n",
    "                        \n",
    "#         X_train.loc[segment, 'ave_roll_mean_' + str(w)] = x_roll_mean.mean()\n",
    "#         X_train.loc[segment, 'std_roll_mean_' + str(w)] = x_roll_mean.std()\n",
    "#         X_train.loc[segment, 'max_roll_mean_' + str(w)] = x_roll_mean.max()\n",
    "#         X_train.loc[segment, 'min_roll_mean_' + str(w)] = x_roll_mean.min()\n",
    "#         #X_train.loc[segment, 'mean_rFFT_roll_mean_' + str(w)] = rfft(x_roll_mean).mean()\n",
    "#         #X_train.loc[segment, 'std_rFFT_roll_mean_' + str(w)] = rfft(x_roll_mean).std()\n",
    "               \n",
    "              \n",
    "#         X_train.loc[segment, 'q01_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.01)\n",
    "#         X_train.loc[segment, 'q05_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.05)\n",
    "#         X_train.loc[segment, 'q10_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.10)\n",
    "#         X_train.loc[segment, 'q95_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.95)\n",
    "#         X_train.loc[segment, 'q99_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.99)\n",
    "#         X_train.loc[segment, 'q01_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.01)\n",
    "#         X_train.loc[segment, 'q05_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.05)\n",
    "#         X_train.loc[segment, 'q95_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.95)\n",
    "#         X_train.loc[segment, 'q99_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.99)\n",
    "#         X_train.loc[segment, 'ave_roll_abs_mean_' + str(w)] = x_roll_abs_mean.mean()\n",
    "#         X_train.loc[segment, 'std_roll_abs_mean_' + str(w)] = x_roll_abs_mean.std()\n",
    "#         X_train.loc[segment, 'max_roll_abs_mean_' + str(w)] = x_roll_abs_mean.max()\n",
    "#         X_train.loc[segment, 'min_roll_abs_mean_' + str(w)] = x_roll_abs_mean.min()\n",
    "#         X_train.loc[segment, 'q01_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.01)\n",
    "#         X_train.loc[segment, 'q05_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.05)\n",
    "#         X_train.loc[segment, 'q95_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.95)\n",
    "#         X_train.loc[segment, 'q99_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.99)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "Y= y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=.3,random_state=100, shuffle=False)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train)\n",
    "X_train_scaled = scl_obj.transform(X_train)\n",
    "scl_obj.fit(X_test)\n",
    "X_test_scaled = scl_obj.transform(X_test)  \n",
    "scl_obj.fit(X)\n",
    "X_scaled = scl_obj.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# X = pd.read_csv('X.csv',index_col = 0)\n",
    "# Y = pd.read_csv('Y.csv',index_col = 0)\n",
    "# X_train = pd.read_csv('X_train.csv',index_col = 0)\n",
    "# X_test = pd.read_csv('X_test.csv',index_col = 0)\n",
    "# y_train = pd.read_csv('y_train.csv',index_col = 0)\n",
    "# y_test = pd.read_csv('y_test.csv',index_col = 0)\n",
    "# X_train_scaled = np.load('X_train_scaled.npy')\n",
    "# X_test_scaled = np.load('X_test_scaled.npy')\n",
    "# X_scaled = np.load('X_scaled.npy')\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/ensemble.html#forest\n",
    "# The main parameters to adjust when using these methods is n_estimators and max_features.\n",
    "#n_estimators is the number of trees in the forest. The larger the better, but also the longer it will take to compute.\n",
    "#In addition, note that results will stop getting significantly better beyond a critical number of trees.\n",
    "#max_features is the size of the random subsets of features to consider when splitting a node.\n",
    "#The lower the greater the reduction of variance, but also the greater the increase in bias.\n",
    "#Empirical good default values are max_features=n_features for regression problems, and\n",
    "#max_features=sqrt(n_features) for classification tasks (where n_features is the number of features in the data).\n",
    "#Good results are often achieved when setting max_depth=None in combination with \n",
    "#min_samples_split=2 (i.e., when fully developing the trees). \n",
    "#Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM.\n",
    "#The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are\n",
    "#used by default (bootstrap=True) while the default strategy for extra-trees is to use the whole dataset (bootstrap=False).\n",
    "#When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples.\n",
    "#This can be enabled by setting oob_score=True.\n",
    "\n",
    "grid = {\n",
    "    'max_depth': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50],\n",
    "    'max_features': ['auto', 'sqrt', 'log2',10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25],\n",
    "    'min_samples_leaf': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],\n",
    "    'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "rfr = RandomForestRegressor(criterion='mse',n_estimators=300,n_jobs=-1)\n",
    "gs1 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs1.fit(X_train_scaled, y_train.values.ravel())  \n",
    "gs1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "# X.to_csv(r'X.csv')\n",
    "# Y.to_csv(r'Y.csv')\n",
    "# X_train.to_csv(r'X_train.csv')\n",
    "# X_test.to_csv(r'X_test.csv')\n",
    "# y_train.to_csv(r'y_train.csv')\n",
    "# y_test.to_csv(r'y_test.csv')\n",
    "# np.save('X_train_scaled', X_train_scaled)\n",
    "# np.save('X_test_scaled', X_test_scaled)\n",
    "# np.save('X_scaled', X_scaled)\n",
    "\n",
    "X.to_csv(r'Xbig.csv')\n",
    "Y.to_csv(r'Ybig.csv')\n",
    "X_train.to_csv(r'X_trainbig.csv')\n",
    "X_test.to_csv(r'X_testbig.csv')\n",
    "y_train.to_csv(r'y_trainbig.csv')\n",
    "y_test.to_csv(r'y_testbig.csv')\n",
    "np.save('X_train_scaledbig', X_train_scaled)\n",
    "np.save('X_test_scaledbig', X_test_scaled)\n",
    "np.save('X_scaledbig', X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs1.predict(X_test_scaled)\n",
    "full_predictions = gs1.predict(X_scaled)\n",
    "\n",
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(figsize=(20,15))\n",
    "ax3 = axis\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 300 estimators\n",
    "# mean_absolute_error: 2.0333497802169322\n",
    "# r2_score: 0.49694656392058145\n",
    "\n",
    "# 500 estimators\n",
    "# mean_absolute_error: 2.033902275585301\n",
    "# r2_score: 0.49555513299363063\n",
    "\n",
    "# # 700 estimators\n",
    "# mean_absolute_error: 2.0392882510864974\n",
    "# r2_score: 0.49099361433577526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model From Disk\n",
    "full_predictions = pickle.load(open('full_predictions.sav', 'rb'))\n",
    "predictions = pickle.load(open('predictions.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model to Disk\n",
    "# filename = 'full_predictions.sav'\n",
    "# pickle.dump(full_predictions, open(filename, 'wb'))\n",
    "# filename = 'predictions.sav'\n",
    "# pickle.dump(predictions, open(filename, 'wb'))\n",
    "np.savetxt(\"y_nolog.csv\", y_nolog, delimiter=\",\")\n",
    "np.savetxt(\"y_nolog_predict.csv\", y_nolog_predict, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "grid = {\n",
    "    'max_depth': [4, 6, 8, 10, 12],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [2, 4, 12],\n",
    "    'min_samples_split': st.randint(10,100),\n",
    "}\n",
    "#params = grid_search_cv(DecisionTreeRegressor(criterion='mae'),\n",
    "                        #grid, X_train_scaled, target)\n",
    "#rf_oof = make_predictions(DecisionTreeRegressor(splitter='random', **params),\n",
    "                         # X_train_scaled, target)\n",
    "rfr = DecisionTreeRegressor(criterion='mae')\n",
    "gs3 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs3.fit(X_train_scaled, y_train.values.ravel())  \n",
    "gs3.best_params_      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs3.predict(X_test_scaled)\n",
    "full_predictions = gs1.predict(X_scaled)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "grid = {\n",
    "    #'max_depth': [4, 6, 8, 10, 12],\n",
    "    #'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    #'min_samples_leaf': [2, 4, 12],\n",
    "    #'min_samples_split': [4,8,12,16,20, 30, 40, 50, 60],\n",
    "    \"learning_rate\": st.uniform(0.01, 0.4),\n",
    "    'loss' : [\"square\"],\n",
    "    #'n_estimators' : st.randint(10000, 20000)\n",
    "}\n",
    "#params = grid_search_cv(AdaBoostRegressor(),\n",
    "                        #grid, X_train_scaled, target)\n",
    "#rf_oof = make_predictions(AdaBoostRegressor(**params),\n",
    "                          #X_train_scaled, target)\n",
    "base = Ridge(alpha=1) \n",
    "rfr = AdaBoostRegressor(n_estimators = 500, base_estimator=base)\n",
    "gs4 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs4.fit(X_train_scaled, y_train.values.ravel())  \n",
    "gs4.best_params_      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs4.predict(X_test_scaled)\n",
    "full_predictions = gs1.predict(X_scaled)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of data is increasing day by day and it is becoming difficult for traditional data science algorithms to give faster results. Light GBM is prefixed as ‘Light’ because of its high speed. Light GBM can handle the large size of data and takes lower memory to run. Another reason of why Light GBM is popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development. [https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "grid = {\n",
    "    'learning_rate': [0.1, 0.05, 0.01, 0.005],\n",
    "    'num_leaves': list(range(8, 92, 4)),\n",
    "    'max_depth': [3, 4, 5, 6, 8, 12, 16, -1],\n",
    "    'feature_fraction': [0.8, 0.85, 0.9, 0.95, 1],\n",
    "    'subsample': [0.8, 0.85, 0.9, 0.95, 1],\n",
    "    'lambda_l1': [0, 0.1, 0.2, 0.4, 0.6, 0.9],\n",
    "    'lambda_l2': [0, 0.1, 0.2, 0.4, 0.6, 0.9],\n",
    "    'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'min_gain_to_split': [0, 0.001, 0.01, 0.1],\n",
    "    'random_seed': st.randint(10, 100),\n",
    "    }\n",
    "#params = grid_search_cv(RandomForestRegressor(criterion='mse', n_estimators=50),\n",
    "                        #grid, X_train_scaled, target)\n",
    "rfr = lgb.LGBMRegressor(n_estimators=10000)\n",
    "gs5 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs5.fit(X_train_scaled, y_train.values.ravel())  \n",
    "gs5.best_params_    \n",
    "#rf_oof = make_predictions(RandomForestRegressor(**params),\n",
    "                          #X_train_scaled, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs5.predict(X_test_scaled)\n",
    "full_predictions = gs1.predict(X_scaled)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "grid = {\n",
    "    'max_depth': [6,11,15,20],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [10,16,20,40],\n",
    "    'min_samples_split': [50,98,150,200],\n",
    "    'n_estimators': [100, 200, 495,1000],\n",
    "    'min_weight_fraction_leaf':  st.uniform(0, 0.5)\n",
    "    }\n",
    "\n",
    "etr = ExtraTreesRegressor(criterion='mae')\n",
    "gs6 = RandomizedSearchCV(etr, param_distributions = grid)  \n",
    "gs6.fit(X_train_scaled, y_train.values.ravel())  \n",
    "gs6.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs6.predict(X_test_scaled)\n",
    "full_predictions = gs6.predict(X_scaled)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The values of this array sum to 1, unless all trees are single node trees consisting of only the root node, \n",
    "#in which case it will be an array of zeros.\n",
    "model = ExtraTreesRegressor(criterion='mse',max_depth=20,max_features='auto',min_samples_leaf=20,min_samples_split=50,\n",
    "                            min_weight_fraction_leaf=0.2677238296443694,n_estimators=200 )\n",
    "model.fit(X_train_scaled, y_train.values.ravel())\n",
    "\n",
    "fold_importance_df = pd.DataFrame()\n",
    "feature_importance_df = pd.DataFrame()\n",
    "fold_importance_df[\"Feature\"] = X_train.columns\n",
    "fold_importance_df[\"importance\"] = model.feature_importances_[:len(X_train.columns)]\n",
    "feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:200].index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,26))\n",
    "sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('ExtraTreesRegressor Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ExtraTreesRegressor_importances.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "principalComponents = pca.fit_transform(X)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', \n",
    "                          'principal component 5', 'principal component 6', 'principal component 7', 'principal component 8',\n",
    "                          'principal component 9','principal component 10'])\n",
    "principalDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PCA_train_final = principalDf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_PCA_train, X_PCA_test, y_train, y_test = train_test_split(X_PCA_train_final,Y,test_size=.3,random_state=100, shuffle=True)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "    \n",
    "scl_obj.fit(X_PCA_train)\n",
    "X_PCA_train = scl_obj.transform(X_PCA_train)\n",
    "scl_obj.fit(X_PCA_test)\n",
    "X_PCA_test = scl_obj.transform(X_PCA_test)\n",
    "scl_obj.fit(X_PCA_train_final)\n",
    "X_PCA = scl_obj.transform(X_PCA_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'max_depth': [8, 10],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [2, 4, 12],\n",
    "    'min_samples_split': [2, 6, 12],\n",
    "    #'n_estimators' : st.randint(10000, 20000),\n",
    "}\n",
    "#params = grid_search_cv(RandomForestRegressor(criterion='mse', n_estimators=50),\n",
    "                        #grid, X_train_scaled, target)\n",
    "rfr = RandomForestRegressor(criterion='mse',n_estimators=1000)\n",
    "gs7 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs7.fit(X_PCA_train, y_train.values.ravel())  \n",
    "gs7.best_params_   \n",
    "#rf_oof = make_predictions(RandomForestRegressor(**params),\n",
    "                          #X_train_scaled, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs7.predict(X_PCA_test)\n",
    "full_predictions = gs7.predict(X_PCA)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "one_to_left = st.beta(10, 1)  \n",
    "from_zero_positive = st.expon(0, 50)\n",
    "\n",
    "params = {  \n",
    "    #\"n_estimators\": st.randint(10000, 20000),\n",
    "    \"max_depth\": st.randint(3, 40),\n",
    "    \"learning_rate\": st.uniform(0.01, 0.4),\n",
    "    \"colsample_bytree\": one_to_left,\n",
    "    \"subsample\": one_to_left,\n",
    "    \"gamma\": st.uniform(0, 10),\n",
    "    'reg_alpha': from_zero_positive,\n",
    "    \"min_child_weight\": from_zero_positive,\n",
    "}\n",
    "\n",
    "xgbreg = XGBRegressor(nthreads=-1,n_estimators=500, cv=5)  \n",
    "gs8 = RandomizedSearchCV(xgbreg, params)  \n",
    "gs8.fit(X_PCA_train, y_train.values.ravel())  \n",
    "gs8.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs8.predict(X_PCA_test)\n",
    "full_predictions = gs8.predict(X_PCA)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The values of this array sum to 1, unless all trees are single node trees consisting of only the root node, \n",
    "#in which case it will be an array of zeros.\n",
    "model = XGBRegressor(nthreads=-1,n_estimators=500,colsample_bytree=0.8385749300087559,gamma=4.711994964749211,\n",
    "                     learning_rate=0.02198803862710416,max_depth=17,min_child_weight=114.35959341213693,\n",
    "                     reg_alpha=59.19676051250868,subsample=0.9732956894126421)\n",
    "model.fit(X_PCA_train, y_train)\n",
    "\n",
    "fold_importance_df = pd.DataFrame()\n",
    "feature_importance_df = pd.DataFrame()\n",
    "fold_importance_df[\"Feature\"] = principalDf.columns\n",
    "fold_importance_df[\"importance\"] = model.feature_importances_[:len(principalDf.columns)]\n",
    "feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:200].index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,26))\n",
    "sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('XGBRegressor Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('XGBRegressor_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "grid = {\n",
    "    'max_depth': st.randint(4,12),\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': st.randint(2,12),\n",
    "    'min_samples_split': st.randint(10,100),\n",
    "}\n",
    "#params = grid_search_cv(DecisionTreeRegressor(criterion='mae'),\n",
    "                        #grid, X_train_scaled, target)\n",
    "#rf_oof = make_predictions(DecisionTreeRegressor(splitter='random', **params),\n",
    "                         # X_train_scaled, target)\n",
    "    \n",
    "rfr = DecisionTreeRegressor(criterion='mae')\n",
    "gs9 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs9.fit(X_PCA_train, y_train.values.ravel())  \n",
    "gs9.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs9.predict(X_PCA_test)\n",
    "full_predictions = gs9.predict(X_PCA)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "grid = {\n",
    "    #'max_depth': [4, 6, 8, 10, 12],\n",
    "    #'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    #'min_samples_leaf': [2, 4, 12],\n",
    "    #'min_samples_split': [4,8,12,16,20, 30, 40, 50, 60],\n",
    "    \"learning_rate\": st.uniform(0.01, 0.4),\n",
    "    'loss' : [\"square\"],\n",
    "    #'n_estimators' : st.randint(10000, 20000)\n",
    "}\n",
    "#params = grid_search_cv(AdaBoostRegressor(),\n",
    "                        #grid, X_train_scaled, target)\n",
    "#rf_oof = make_predictions(AdaBoostRegressor(**params),\n",
    "                          #X_train_scaled, target)\n",
    "base = Ridge(alpha=1) \n",
    "rfr = AdaBoostRegressor(n_estimators = 500, base_estimator=base)\n",
    "gs10 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs10.fit(X_PCA_train, y_train.values.ravel())  \n",
    "gs10.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs10.predict(X_PCA_test)\n",
    "full_predictions = gs10.predict(X_PCA)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import scipy.stats as st\n",
    "\n",
    "grid = {\n",
    "     'learning_rate': [0.1, 0.05, 0.01, 0.005],\n",
    "    'num_leaves': list(range(8, 92, 4)),\n",
    "    'max_depth': [3, 4, 5, 6, 8, 12, 16, -1],\n",
    "    'feature_fraction': [0.8, 0.85, 0.9, 0.95, 1],\n",
    "    'subsample': [0.8, 0.85, 0.9, 0.95, 1],\n",
    "    'lambda_l1': [0, 0.1, 0.2, 0.4, 0.6, 0.9],\n",
    "    'lambda_l2': [0, 0.1, 0.2, 0.4, 0.6, 0.9],\n",
    "    'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'min_gain_to_split': [0, 0.001, 0.01, 0.1],\n",
    "    'random_seed': st.randint(10, 100),\n",
    "    }\n",
    "#params = grid_search_cv(RandomForestRegressor(criterion='mse', n_estimators=50),\n",
    "                        #grid, X_train_scaled, target)\n",
    "rfr = lgb.LGBMRegressor(n_estimators=10000)\n",
    "gs11 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs11.fit(X_PCA_train, y_train.values.ravel())  \n",
    "gs11.best_params_    \n",
    "#rf_oof = make_predictions(RandomForestRegressor(**params),\n",
    "                          #X_train_scaled, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs11.predict(X_PCA_test)\n",
    "full_predictions = gs11.predict(X_PCA)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "grid = {\n",
    "    'max_depth': [6,11,15,20],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [10,16,20,40],\n",
    "    'min_samples_split': [50,98,150,200],\n",
    "    'n_estimators': [100, 200, 495,1000],\n",
    "    'min_weight_fraction_leaf':  st.uniform(0, 0.5)\n",
    "    }\n",
    "\n",
    "etr = ExtraTreesRegressor(criterion='mae')\n",
    "gs12 = RandomizedSearchCV(etr, param_distributions = grid)  \n",
    "gs12.fit(X_PCA_train, y_train.values.ravel())  \n",
    "gs12.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs12.predict(X_PCA_test)\n",
    "full_predictions = gs12.predict(X_PCA)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "X_test = pd.DataFrame(columns=X.columns, dtype=np.float64, index=submission.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg_id in X_test.index:\n",
    "    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "    \n",
    "    x = seg['acoustic_data']  # pd series\n",
    "            \n",
    "    I_ave10 = 0.1*len(x)\n",
    "    II_ave10 = 0.2*len(x)\n",
    "    III_ave10 = 0.3*len(x)\n",
    "    IV_ave10 = 0.4*len(x)\n",
    "    V_ave10 = 0.5*len(x)\n",
    "    VI_ave10 = 0.6*len(x)\n",
    "    VII_ave10 = 0.7*len(x)\n",
    "    VIII_ave10 = 0.8*len(x)\n",
    "    IX_ave10 = 0.9*len(x)\n",
    "    \n",
    "    X_test.loc[seg_id, 'rFFT mean'] = rfft(x).mean()\n",
    "    X_test.loc[seg_id, 'rFFT std'] = rfft(x).std()\n",
    "    \n",
    "    X_test.loc[seg_id, '0<x<5'] = sum(x.between(0, 5))\n",
    "    X_test.loc[seg_id, '5<x<10'] = sum(x.between(5, 10))    \n",
    "    X_test.loc[seg_id, '10<x<20'] = sum(x.between(10, 20))\n",
    "    X_test.loc[seg_id, '20<x<30'] = sum(x.between(20, 30))\n",
    "    X_test.loc[seg_id, '30<x<100'] = sum(x.between(30, 100))\n",
    "    X_test.loc[seg_id, '100<x<500'] = sum(x.between(100, 500))\n",
    "    X_test.loc[seg_id, '500<x<1000'] = sum(x.between(500, 1000))\n",
    "    X_test.loc[seg_id, '1000<x<5000'] = sum(x.between(1000, 5000))\n",
    "    \n",
    "    X_test.loc[seg_id, 'skewness'] = moment(x, moment=3)\n",
    "    X_test.loc[seg_id, 'kurtosis'] = moment(x, moment=4)\n",
    "    X_test.loc[seg_id, 'hypermoment'] = moment(x, moment=5)\n",
    "    X_test.loc[seg_id, 'hypermoment2'] = moment(x, moment=6)\n",
    "    X_test.loc[seg_id, 'ave'] = x.values.mean()\n",
    "    X_test.loc[seg_id, 'std'] = x.values.std()\n",
    "    X_test.loc[seg_id, 'max'] = x.values.max()\n",
    "    X_test.loc[seg_id, 'min'] = x.values.min()\n",
    "        \n",
    "    X_test.loc[seg_id, 'q90'] = np.quantile(x.values, 0.90)\n",
    "    X_test.loc[seg_id, 'q95'] = np.quantile(x.values, 0.95)\n",
    "    X_test.loc[seg_id, 'q99'] = np.quantile(x.values, 0.99)\n",
    "    X_test.loc[seg_id, 'q05'] = np.quantile(x.values, 0.05)\n",
    "    X_test.loc[seg_id, 'q10'] = np.quantile(x.values, 0.10)\n",
    "    X_test.loc[seg_id, 'q01'] = np.quantile(x.values, 0.01)\n",
    "    \n",
    "    X_test.loc[seg_id, 'abs_max'] = np.abs(x.values).max()\n",
    "    X_test.loc[seg_id, 'abs_mean'] = np.abs(x.values).mean()\n",
    "    X_test.loc[seg_id, 'abs_std'] = np.abs(x.values).std()\n",
    "        \n",
    "     # New features - rolling features\n",
    "    for w in [100, 1000, 10000]:\n",
    "        x_roll_std = x.rolling(w).std().dropna().values\n",
    "        x_roll_mean = x.rolling(w).mean().dropna().values\n",
    "        x_roll_abs_mean = x.abs().rolling(w).mean().dropna().values\n",
    "        \n",
    "        X_test.loc[seg_id, 'ave_roll_std_' + str(w)] = x_roll_std.mean()\n",
    "        X_test.loc[seg_id, 'std_roll_std_' + str(w)] = x_roll_std.std()\n",
    "        X_test.loc[seg_id, 'max_roll_std_' + str(w)] = x_roll_std.max()\n",
    "        X_test.loc[seg_id, 'min_roll_std_' + str(w)] = x_roll_std.min()\n",
    "                \n",
    "        X_test.loc[seg_id, 'ave_roll_mean_' + str(w)] = x_roll_mean.mean()\n",
    "        X_test.loc[seg_id, 'std_roll_mean_' + str(w)] = x_roll_mean.std()\n",
    "        X_test.loc[seg_id, 'max_roll_mean_' + str(w)] = x_roll_mean.max()\n",
    "        X_test.loc[seg_id, 'min_roll_mean_' + str(w)] = x_roll_mean.min()\n",
    "        \n",
    "        X_test.loc[seg_id, 'q01_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.01)\n",
    "        X_test.loc[seg_id, 'q05_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.05)\n",
    "        X_test.loc[seg_id, 'q10_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.10)\n",
    "        X_test.loc[seg_id, 'q95_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.95)\n",
    "        X_test.loc[seg_id, 'q99_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.99)\n",
    "        X_test.loc[seg_id, 'q01_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.01)\n",
    "        X_test.loc[seg_id, 'q05_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.05)\n",
    "        X_test.loc[seg_id, 'q95_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.95)\n",
    "        X_test.loc[seg_id, 'q99_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.99)\n",
    "        X_test.loc[seg_id, 'ave_roll_abs_mean_' + str(w)] = x_roll_abs_mean.mean()\n",
    "        X_test.loc[seg_id, 'std_roll_abs_mean_' + str(w)] = x_roll_abs_mean.std()\n",
    "        X_test.loc[seg_id, 'max_roll_abs_mean_' + str(w)] = x_roll_abs_mean.max()\n",
    "        X_test.loc[seg_id, 'min_roll_abs_mean_' + str(w)] = x_roll_abs_mean.min()\n",
    "        X_test.loc[seg_id, 'q01_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.01)\n",
    "        X_test.loc[seg_id, 'q05_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.05)\n",
    "        X_test.loc[seg_id, 'q95_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.95)\n",
    "        X_test.loc[seg_id, 'q99_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scl_obj.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gs6.predict(X_test_scaled)\n",
    "submission['time_to_failure'] = np.square(predictions)\n",
    "submission[submission < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_ExtraTreesRegressor.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
