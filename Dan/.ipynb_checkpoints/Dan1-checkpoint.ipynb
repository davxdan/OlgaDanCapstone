{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABLCAYAAABHlv13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAAiJJREFUeJzt2zGKVEEUheFbozCCiaADiqBopGBmb8PERfQaDAVzjTRwA27FwElEMNEFqIOhoiBloImJb5qhqNOP78saKjg3+YMH3XrvBcB8B7MHAPCHIAOEEGSAEIIMEEKQAUIIMkAIQQYIIcgAIQQZIMT5pQettW1Vbauq6uDwfl24NnrTNHd+fJo9YahvN67OnjDMpa9fZk8Y6kM/nD1hqH793OwJQ31///mk93609K7t8tfpdvFWr7uPz7Ir2uuPT2dPGOr4+aPZE4Z5+OrF7AlDPfh1e/aEoX4+uTx7wlBvN8+Oe++bpXc+WQCEEGSAEIIMEEKQAUIIMkAIQQYIIcgAIQQZIIQgA4QQZIAQggwQQpABQggyQAhBBgghyAAhBBkghCADhBBkgBCCDBBCkAFCCDJACEEGCCHIACEEGSCEIAOEEGSAEIIMEEKQAUIIMkAIQQYIIcgAIQQZIIQgA4QQZIAQggwQQpABQggyQAhBBgghyAAhBBkghCADhBBkgBCCDBBCkAFCCDJACEEGCCHIACEEGSCEIAOEEGSAEIIMEEKQAUK03vv/H7S2rart35/3qurd6FETXamqk9kjBlnzbVXu23drv+9m7/1o6dFikP953Nqb3vvmTLOCrfm+Nd9W5b59t/b7TssnC4AQggwQYtcgvxyyIsea71vzbVXu23drv+9UdvqGDMA4PlkAhBBkgBCCDBBCkAFCCDJAiN8G0FUhkBLTZAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, time, warnings, math, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import moment\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.fftpack import fft, rfft\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import mean_absolute_error,r2_score\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.set_option(\"display.precision\", 14)\n",
    "\n",
    "path=\"C:/Users/danie/Documents/GitHub/OlgaDanCapstone/GPUProject\"\n",
    "os.chdir(path)\n",
    "flatui = [\"#0033A0\", \"#C8102E\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "sns.set_palette(flatui)\n",
    "sns.palplot(sns.color_palette())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "7abc992d4c03d57c543d6fc41fefc9e008d401dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train = pd.read_csv('train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\n",
    "train = pd.read_csv('1st100000000.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\n",
    "train = train[['acoustic_data','time_to_failure']]\n",
    "\n",
    "\n",
    "# test_file = pd.read_csv('seg_00a37e.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acoustic_data</th>\n",
       "      <th>time_to_failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>29.00000000000000</td>\n",
       "      <td>29.00000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3333.72413793103442</td>\n",
       "      <td>0.31605207920074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>264.55432827840383</td>\n",
       "      <td>0.00059250672348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3012.00000000000000</td>\n",
       "      <td>0.31559649109840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3124.00000000000000</td>\n",
       "      <td>0.31559655070305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3240.00000000000000</td>\n",
       "      <td>0.31559774279594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3466.00000000000000</td>\n",
       "      <td>0.31679680943489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3914.00000000000000</td>\n",
       "      <td>0.31679698824883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             acoustic_data    time_to_failure\n",
       "count    29.00000000000000  29.00000000000000\n",
       "mean   3333.72413793103442   0.31605207920074\n",
       "std     264.55432827840383   0.00059250672348\n",
       "min    3012.00000000000000   0.31559649109840\n",
       "25%    3124.00000000000000   0.31559655070305\n",
       "50%    3240.00000000000000   0.31559774279594\n",
       "75%    3466.00000000000000   0.31679680943489\n",
       "max    3914.00000000000000   0.31679698824883"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "# train['acoustic_data'].std() = 10.735707249510964\n",
    "# train['acoustic_data'].mean() = 4.519467573700124\n",
    "# train['acoustic_data'].min() = -5515\n",
    "# train['acoustic_data'].max() = 5444\n",
    "# train['time_to_failure'].mean() = 5.678285\n",
    "# train['time_to_failure'].std() = 3.6726966\n",
    "# train['time_to_failure'].min() = .000095503965\n",
    "# train['time_to_failure'].max() = 16.1074\n",
    "\n",
    "# type(train)\n",
    "# train.describe()\n",
    "# train.head()\n",
    "# train.columns\n",
    "# train.acoustic_data.head()\n",
    "# showme = train[train.acoustic_data > 3000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# &,|,~,^,df.any(),df.all() \n",
    "# Logical and, or, not, xor, any, all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/ensemble.html#forest\n",
    "# The main parameters to adjust when using these methods is n_estimators and max_features.\n",
    "#n_estimators is the number of trees in the forest. The larger the better, but also the longer it will take to compute.\n",
    "#In addition, note that results will stop getting significantly better beyond a critical number of trees.\n",
    "#max_features is the size of the random subsets of features to consider when splitting a node.\n",
    "#The lower the greater the reduction of variance, but also the greater the increase in bias.\n",
    "#Empirical good default values are max_features=n_features for regression problems, and\n",
    "#max_features=sqrt(n_features) for classification tasks (where n_features is the number of features in the data).\n",
    "#Good results are often achieved when setting max_depth=None in combination with \n",
    "#min_samples_split=2 (i.e., when fully developing the trees). \n",
    "#Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM.\n",
    "#The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are\n",
    "#used by default (bootstrap=True) while the default strategy for extra-trees is to use the whole dataset (bootstrap=False).\n",
    "#When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples.\n",
    "#This can be enabled by setting oob_score=True.\n",
    "\n",
    "\n",
    "# https://agupubs.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1002%2F2017GL074677&file=grl56367-sup-0001-supinfo.pdf\n",
    "\n",
    "#An accelerometer records the acoustic emission (AE) emanating from the shearing layers. \n",
    "#The shear stress imposed by the driving block is also monitored as well as other physical parameters such\n",
    "#as the shearing rate, gouge layer thickness, friction, and the applied load\n",
    "\n",
    "\n",
    "# The minimum number of samples to generate a split was 30.\n",
    "# The maximum number of features to consider when making a split was 40 (out of 100 features).\n",
    "# The forest size was 1000 trees. The performance of the random forest is not sensitive to this choice of\n",
    "#hyper-parameters: changing any hyper parameter by a factor of 2 typically affects the R2 performance by\n",
    "#only a few percent.\n",
    "\n",
    "# To create a model that uncovers the physics of shear failure, we make predictions\n",
    "# 119 using moving time windows applied to the data. Each window is 1.8s, which is small\n",
    "# 120 compared to the time between fault gouge failures (8s on average). The offset between\n",
    "# 121 windows is 0.18s, meaning that consecutive time windows overlap by 90 percent. We\n",
    "# 122 characterize the acoustical signal in each window by a set of ≈ 50 statistical features (de123 tailed in section “Statistical feature”). Each window is further split in two, and the features are computed for each sub-window to form one data point xi\n",
    "# 124 , totaling ≈ 100 statis125 tical features. We then label the data point xi according to the time remaining until the\n",
    "# next gouge failure, yi\n",
    "# 126 , determined from the stress signal.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# At each node, we select a random subset of 40\n",
    "# 69 percent of the available features.\n",
    "\n",
    "# We compute\n",
    "# 112 regularization hyper-parameters by grid search based on a 3-fold cross-validation.\n",
    "\n",
    "# grid = {\n",
    "#     'max_depth': [10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40],\n",
    "#     'max_features': ['auto', 'sqrt', 'log2'],\n",
    "#     'min_samples_leaf': [10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],\n",
    "# #     'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],\n",
    "#       'min_samples_split': [30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50],\n",
    "#     'bootstrap': [True]\n",
    "# }\n",
    "\n",
    "grid = {\n",
    "    'max_depth': [8, 10],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [2, 4, 12],\n",
    "    'min_samples_split': [2, 6, 12],\n",
    "    }\n",
    "\n",
    "# rfr = RandomForestRegressor(criterion='mse',n_estimators=1000,n_jobs=-1)\n",
    "rfr = RandomForestRegressor(criterion='mse',n_estimators=1000,n_jobs=-1)\n",
    "gs1 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs1.fit(X_train_scaled, y_train.values.ravel())  \n",
    "gs1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs1.predict(X_test_scaled)\n",
    "full_predictions = gs1.predict(X_scaled)\n",
    "\n",
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(figsize=(24,16),)\n",
    "ax3 = axis\n",
    "ax3.set_xlabel('Train Versus Test')\n",
    "ax3.set_ylabel('Seconds Before Failire')\n",
    "ax3.plot(y_nolog_predict, color='#C8102E')\n",
    "ax3.plot(y_nolog, color='#0033A0')\n",
    "red_patch = mpatches.Patch(color='#C8102E', label='Predicted')\n",
    "blue_patch = mpatches.Patch(color='#0033A0', label='Actual')\n",
    "plt.legend(handles=[red_patch,blue_patch],loc=(0.01, 0.93))\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_ad_sample_df = train['acoustic_data'].values[::100]\n",
    "# train_ttf_sample_df = train['time_to_failure'].values[::100]\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "train_ad_sample_df = train['acoustic_data']\n",
    "train_ttf_sample_df = train['time_to_failure']\n",
    "\n",
    "def plot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df, title=\"Laboratory Earthquake Data: 629 Million Observations\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(24,16))\n",
    "    plt.title(title)\n",
    "    plt.plot(train_ad_sample_df, color='#0033A0')\n",
    "    ax1.set_ylabel('Signal')\n",
    "    #plt.legend(['acoustic data'], loc=(0.01, 0.95))\n",
    "    ax2 = ax1.twinx()\n",
    "    plt.plot(train_ttf_sample_df, color='#C8102E')\n",
    "    ax2.set_ylabel('Seconds')\n",
    "    #plt.legend(['time to failure'], loc=(0.01, 0.9))\n",
    "    red_patch = mpatches.Patch(color='#C8102E', label='Seconds Remaining Until Failure')\n",
    "    blue_patch = mpatches.Patch(color='#0033A0', label='Acoustic Signal')\n",
    "    plt.legend(handles=[red_patch,blue_patch],loc=(0.01, 0.93))\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "# X.to_csv(r'X.csv')\n",
    "# Y.to_csv(r'Y.csv')\n",
    "# X_train.to_csv(r'X_train.csv')\n",
    "# X_test.to_csv(r'X_test.csv')\n",
    "# y_train.to_csv(r'y_train.csv')\n",
    "# y_test.to_csv(r'y_test.csv')\n",
    "# np.save('X_train_scaled', X_train_scaled)\n",
    "# np.save('X_test_scaled', X_test_scaled)\n",
    "# np.save('X_scaled', X_scaled)\n",
    "\n",
    "# acoustic_data = train['acoustic_data']   # pd series\n",
    "# time_to_failure_data = train['time_to_failure']\n",
    "# acoustic_data = acoustic_data[:150000]\n",
    "# time_to_failure_data = time_to_failure_data[:150000]\n",
    "\n",
    "# acoustic_data.to_csv('acoustic_data.csv')\n",
    "# time_to_failure_data.to_csv('time_to_failure_data.csv')\n",
    "\n",
    "\n",
    "# temp1=train[0:100000000]\n",
    "# temp2=train[100000000:200000000]\n",
    "# temp3=train[200000000:300000000]\n",
    "# temp4=train[300000000:400000000]\n",
    "# temp5=train[400000000:500000000]\n",
    "# temp6=train[500000000:600000000]\n",
    "# temp7=train[600000000:]\n",
    "\n",
    "\n",
    "temp1.to_csv(r'1st100000000.csv')\n",
    "temp2.to_csv(r'2nd100000000.csv')\n",
    "temp3.to_csv(r'3rd100000000.csv')\n",
    "temp4.to_csv(r'4th100000000.csv')\n",
    "temp5.to_csv(r'5th100000000.csv')\n",
    "temp6.to_csv(r'6th100000000.csv')\n",
    "temp7.to_csv(r'7th100000000.csv')\n",
    "# train.shape\n",
    "# temp7.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model From Disk\n",
    "full_predictions = pickle.load(open('full_predictions.sav', 'rb'))\n",
    "predictions = pickle.load(open('predictions.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model to Disk\n",
    "filename = 'full_predictions.sav'\n",
    "pickle.dump(full_predictions, open(filename, 'wb'))\n",
    "filename = 'predictions.sav'\n",
    "pickle.dump(predictions, open(filename, 'wb'))\n",
    "np.savetxt(\"y_nolog.csv\", y_nolog, delimiter=\",\")\n",
    "np.savetxt(\"y_nolog_predict.csv\", y_nolog_predict, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
