{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from scipy.fftpack import fft, rfft\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import moment\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "import scipy.stats as st\n",
    "import pickle\n",
    "sns.set()\n",
    "path=\"C:/Users/danie/Documents/GitHub/OlgaDanCapstone/GPUProject\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "7abc992d4c03d57c543d6fc41fefc9e008d401dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "db0ef041c84fcb1f60fdea9450d3fa8476cf4b58",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acoustic_data</th>\n",
       "      <th>time_to_failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>1.469099998474121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1.469099998474121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>1.469099998474121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1.469099998474121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1.469099998474121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acoustic_data    time_to_failure\n",
       "0             12  1.469099998474121\n",
       "1              6  1.469099998474121\n",
       "2              8  1.469099998474121\n",
       "3              5  1.469099998474121\n",
       "4              8  1.469099998474121"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.precision\", 15)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d34d5f3273f5dc161aa8207fbb503f14684728c"
   },
   "source": [
    "There are outliers in both directions; let's try to plot the same distribution with x in the range -20 to 30. The black line is the closest normal distribution (gaussian) possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: We will divide the train data set for segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the training data in chunks of 150,000 examples and extract the following features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            1.469099998474121\n",
       "1            1.469099998474121\n",
       "2            1.469099998474121\n",
       "3            1.469099998474121\n",
       "4            1.469099998474121\n",
       "5            1.469099998474121\n",
       "6            1.469099998474121\n",
       "7            1.469099998474121\n",
       "8            1.469099998474121\n",
       "9            1.469099998474121\n",
       "10           1.469099998474121\n",
       "11           1.469099998474121\n",
       "12           1.469099998474121\n",
       "13           1.469099998474121\n",
       "14           1.469099998474121\n",
       "15           1.469099998474121\n",
       "16           1.469099998474121\n",
       "17           1.469099998474121\n",
       "18           1.469099998474121\n",
       "19           1.469099998474121\n",
       "20           1.469099998474121\n",
       "21           1.469099998474121\n",
       "22           1.469099998474121\n",
       "23           1.469099998474121\n",
       "24           1.469099998474121\n",
       "25           1.469099998474121\n",
       "26           1.469099998474121\n",
       "27           1.469099998474121\n",
       "28           1.469099998474121\n",
       "29           1.469099998474121\n",
       "                   ...        \n",
       "629145450    9.759795188903809\n",
       "629145451    9.759795188903809\n",
       "629145452    9.759795188903809\n",
       "629145453    9.759795188903809\n",
       "629145454    9.759795188903809\n",
       "629145455    9.759795188903809\n",
       "629145456    9.759795188903809\n",
       "629145457    9.759795188903809\n",
       "629145458    9.759795188903809\n",
       "629145459    9.759795188903809\n",
       "629145460    9.759795188903809\n",
       "629145461    9.759795188903809\n",
       "629145462    9.759795188903809\n",
       "629145463    9.759795188903809\n",
       "629145464    9.759795188903809\n",
       "629145465    9.759795188903809\n",
       "629145466    9.759795188903809\n",
       "629145467    9.759795188903809\n",
       "629145468    9.759795188903809\n",
       "629145469    9.759795188903809\n",
       "629145470    9.759795188903809\n",
       "629145471    9.759795188903809\n",
       "629145472    9.759795188903809\n",
       "629145473    9.759795188903809\n",
       "629145474    9.759795188903809\n",
       "629145475    9.759795188903809\n",
       "629145476    9.759795188903809\n",
       "629145477    9.759795188903809\n",
       "629145478    9.759795188903809\n",
       "629145479    9.759795188903809\n",
       "Name: time_to_failure, Length: 629145480, dtype: float32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train['acoustic_data']\n",
    "Y = train['time_to_failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440401836,)\n",
      "(188743644,)\n",
      "(440401836,)\n",
      "(188743644,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 5.  5.  8. ... -1.  2. 10.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-33f251fd6e69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mscl_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mscl_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mX_train_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscl_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mscl_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    661\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[0;32m    662\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[1;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    519\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 5.  5.  8. ... -1.  2. 10.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=.3,random_state=100, shuffle=True)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train)\n",
    "X_train_scaled = scl_obj.transform(X_train)\n",
    "scl_obj.fit(X_test)\n",
    "X_test_scaled = scl_obj.transform(X_test)  \n",
    "scl_obj.fit(X)\n",
    "X_scaled = scl_obj.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "X = pd.read_csv('X.csv',index_col = 0)\n",
    "Y = pd.read_csv('Y.csv',index_col = 0)\n",
    "X_train = pd.read_csv('X_train.csv',index_col = 0)\n",
    "X_test = pd.read_csv('X_test.csv',index_col = 0)\n",
    "y_train = pd.read_csv('y_train.csv',index_col = 0)\n",
    "y_test = pd.read_csv('y_test.csv',index_col = 0)\n",
    "X_train_scaled = np.load('X_train_scaled.npy')\n",
    "X_test_scaled = np.load('X_test_scaled.npy')\n",
    "X_scaled = np.load('X_scaled.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'max_depth': [8, 10],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [2, 4, 12],\n",
    "    'min_samples_split': [2, 6, 12],\n",
    "    }\n",
    "rfr = RandomForestRegressor(criterion='mse',n_estimators=1000,n_jobs=-1)\n",
    "gs1 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs1.fit(X_train_scaled, y_train.values.ravel())  \n",
    "gs1.best_params_    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "# X.to_csv(r'X.csv')\n",
    "# Y.to_csv(r'Y.csv')\n",
    "# X_train.to_csv(r'X_train.csv')\n",
    "# X_test.to_csv(r'X_test.csv')\n",
    "# y_train.to_csv(r'y_train.csv')\n",
    "# y_test.to_csv(r'y_test.csv')\n",
    "# np.save('X_train_scaled', X_train_scaled)\n",
    "# np.save('X_test_scaled', X_test_scaled)\n",
    "# np.save('X_scaled', X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions =  gs1.predict(X_test_scaled)\n",
    "full_predictions = gs1.predict(X_scaled)\n",
    "\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dt = 0.01\n",
    "# t = np.arange(0, 30, dt)\n",
    "# nse1 = np.random.randn(len(t))                 # white noise 1\n",
    "# nse2 = np.random.randn(len(t))                 # white noise 2\n",
    "\n",
    "# # Two signals with a coherent part at 10Hz and a random part\n",
    "# s1 = np.sin(2 * np.pi * 10 * t) + nse1\n",
    "# s2 = np.sin(2 * np.pi * 10 * t) + nse2\n",
    "\n",
    "# fig, axs = plt.subplots(2, 1)\n",
    "# axs[0].plot(t, s1, t, s2)\n",
    "# axs[0].set_xlim(0, 2)\n",
    "# axs[0].set_xlabel('time')\n",
    "# axs[0].set_ylabel('s1 and s2')\n",
    "# axs[0].grid(True)\n",
    "\n",
    "# cxy, f = axs[1].cohere(s1, s2, 256, 1. / dt)\n",
    "# axs[1].set_ylabel('coherence')\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(figsize=(20,15))\n",
    "ax3 = axis\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model From Disk\n",
    "full_predictions = pickle.load(open('full_predictions.sav', 'rb'))\n",
    "predictions = pickle.load(open('predictions.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model to Disk\n",
    "# filename = 'full_predictions.sav'\n",
    "# pickle.dump(full_predictions, open(filename, 'wb'))\n",
    "# filename = 'predictions.sav'\n",
    "# pickle.dump(predictions, open(filename, 'wb'))\n",
    "np.savetxt(\"y_nolog.csv\", y_nolog, delimiter=\",\")\n",
    "np.savetxt(\"y_nolog_predict.csv\", y_nolog_predict, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "one_to_left = st.beta(10, 1)  \n",
    "from_zero_positive = st.expon(0, 50)\n",
    "\n",
    "params = {  \n",
    "    #\"n_estimators\": st.randint(10000, 20000),\n",
    "    \"max_depth\": st.randint(3, 40),\n",
    "    \"learning_rate\": st.uniform(0.01, 0.4),\n",
    "    \"colsample_bytree\": one_to_left,\n",
    "    \"subsample\": one_to_left,\n",
    "    \"gamma\": st.uniform(0, 10),\n",
    "    'reg_alpha': from_zero_positive,\n",
    "    \"min_child_weight\": from_zero_positive,\n",
    "}\n",
    "xgbreg = XGBRegressor(nthreads=-1, n_estimators=1000)  \n",
    "gs2 = RandomizedSearchCV(xgbreg, params)  \n",
    "gs2.fit(X_train_scaled, y_train.values.ravel())  \n",
    "gs2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs2.predict(X_test_scaled)\n",
    "full_predictions = gs1.predict(X_scaled)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "grid = {\n",
    "    'max_depth': [4, 6, 8, 10, 12],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [2, 4, 12],\n",
    "    'min_samples_split': st.randint(10,100),\n",
    "}\n",
    "#params = grid_search_cv(DecisionTreeRegressor(criterion='mae'),\n",
    "                        #grid, X_train_scaled, target)\n",
    "#rf_oof = make_predictions(DecisionTreeRegressor(splitter='random', **params),\n",
    "                         # X_train_scaled, target)\n",
    "rfr = DecisionTreeRegressor(criterion='mae')\n",
    "gs3 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs3.fit(X_train_scaled, y_train.values.ravel())  \n",
    "gs3.best_params_      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs3.predict(X_test_scaled)\n",
    "full_predictions = gs1.predict(X_scaled)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "grid = {\n",
    "    #'max_depth': [4, 6, 8, 10, 12],\n",
    "    #'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    #'min_samples_leaf': [2, 4, 12],\n",
    "    #'min_samples_split': [4,8,12,16,20, 30, 40, 50, 60],\n",
    "    \"learning_rate\": st.uniform(0.01, 0.4),\n",
    "    'loss' : [\"square\"],\n",
    "    #'n_estimators' : st.randint(10000, 20000)\n",
    "}\n",
    "#params = grid_search_cv(AdaBoostRegressor(),\n",
    "                        #grid, X_train_scaled, target)\n",
    "#rf_oof = make_predictions(AdaBoostRegressor(**params),\n",
    "                          #X_train_scaled, target)\n",
    "base = Ridge(alpha=1) \n",
    "rfr = AdaBoostRegressor(n_estimators = 500, base_estimator=base)\n",
    "gs4 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs4.fit(X_train_scaled, y_train.values.ravel())  \n",
    "gs4.best_params_      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs4.predict(X_test_scaled)\n",
    "full_predictions = gs1.predict(X_scaled)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of data is increasing day by day and it is becoming difficult for traditional data science algorithms to give faster results. Light GBM is prefixed as ‘Light’ because of its high speed. Light GBM can handle the large size of data and takes lower memory to run. Another reason of why Light GBM is popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development. [https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "grid = {\n",
    "    'learning_rate': [0.1, 0.05, 0.01, 0.005],\n",
    "    'num_leaves': list(range(8, 92, 4)),\n",
    "    'max_depth': [3, 4, 5, 6, 8, 12, 16, -1],\n",
    "    'feature_fraction': [0.8, 0.85, 0.9, 0.95, 1],\n",
    "    'subsample': [0.8, 0.85, 0.9, 0.95, 1],\n",
    "    'lambda_l1': [0, 0.1, 0.2, 0.4, 0.6, 0.9],\n",
    "    'lambda_l2': [0, 0.1, 0.2, 0.4, 0.6, 0.9],\n",
    "    'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'min_gain_to_split': [0, 0.001, 0.01, 0.1],\n",
    "    'random_seed': st.randint(10, 100),\n",
    "    }\n",
    "#params = grid_search_cv(RandomForestRegressor(criterion='mse', n_estimators=50),\n",
    "                        #grid, X_train_scaled, target)\n",
    "rfr = lgb.LGBMRegressor(n_estimators=10000)\n",
    "gs5 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs5.fit(X_train_scaled, y_train.values.ravel())  \n",
    "gs5.best_params_    \n",
    "#rf_oof = make_predictions(RandomForestRegressor(**params),\n",
    "                          #X_train_scaled, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs5.predict(X_test_scaled)\n",
    "full_predictions = gs1.predict(X_scaled)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "grid = {\n",
    "    'max_depth': [6,11,15,20],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [10,16,20,40],\n",
    "    'min_samples_split': [50,98,150,200],\n",
    "    'n_estimators': [100, 200, 495,1000],\n",
    "    'min_weight_fraction_leaf':  st.uniform(0, 0.5)\n",
    "    }\n",
    "\n",
    "etr = ExtraTreesRegressor(criterion='mae')\n",
    "gs6 = RandomizedSearchCV(etr, param_distributions = grid)  \n",
    "gs6.fit(X_train_scaled, y_train.values.ravel())  \n",
    "gs6.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs6.predict(X_test_scaled)\n",
    "full_predictions = gs6.predict(X_scaled)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The values of this array sum to 1, unless all trees are single node trees consisting of only the root node, \n",
    "#in which case it will be an array of zeros.\n",
    "model = ExtraTreesRegressor(criterion='mse',max_depth=20,max_features='auto',min_samples_leaf=20,min_samples_split=50,\n",
    "                            min_weight_fraction_leaf=0.2677238296443694,n_estimators=200 )\n",
    "model.fit(X_train_scaled, y_train.values.ravel())\n",
    "\n",
    "fold_importance_df = pd.DataFrame()\n",
    "feature_importance_df = pd.DataFrame()\n",
    "fold_importance_df[\"Feature\"] = X_train.columns\n",
    "fold_importance_df[\"importance\"] = model.feature_importances_[:len(X_train.columns)]\n",
    "feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:200].index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,26))\n",
    "sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('ExtraTreesRegressor Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ExtraTreesRegressor_importances.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "principalComponents = pca.fit_transform(X)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', \n",
    "                          'principal component 5', 'principal component 6', 'principal component 7', 'principal component 8',\n",
    "                          'principal component 9','principal component 10'])\n",
    "principalDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PCA_train_final = principalDf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_PCA_train, X_PCA_test, y_train, y_test = train_test_split(X_PCA_train_final,Y,test_size=.3,random_state=100, shuffle=True)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "    \n",
    "scl_obj.fit(X_PCA_train)\n",
    "X_PCA_train = scl_obj.transform(X_PCA_train)\n",
    "scl_obj.fit(X_PCA_test)\n",
    "X_PCA_test = scl_obj.transform(X_PCA_test)\n",
    "scl_obj.fit(X_PCA_train_final)\n",
    "X_PCA = scl_obj.transform(X_PCA_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'max_depth': [8, 10],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [2, 4, 12],\n",
    "    'min_samples_split': [2, 6, 12],\n",
    "    #'n_estimators' : st.randint(10000, 20000),\n",
    "}\n",
    "#params = grid_search_cv(RandomForestRegressor(criterion='mse', n_estimators=50),\n",
    "                        #grid, X_train_scaled, target)\n",
    "rfr = RandomForestRegressor(criterion='mse',n_estimators=1000)\n",
    "gs7 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs7.fit(X_PCA_train, y_train.values.ravel())  \n",
    "gs7.best_params_   \n",
    "#rf_oof = make_predictions(RandomForestRegressor(**params),\n",
    "                          #X_train_scaled, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs7.predict(X_PCA_test)\n",
    "full_predictions = gs7.predict(X_PCA)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "one_to_left = st.beta(10, 1)  \n",
    "from_zero_positive = st.expon(0, 50)\n",
    "\n",
    "params = {  \n",
    "    #\"n_estimators\": st.randint(10000, 20000),\n",
    "    \"max_depth\": st.randint(3, 40),\n",
    "    \"learning_rate\": st.uniform(0.01, 0.4),\n",
    "    \"colsample_bytree\": one_to_left,\n",
    "    \"subsample\": one_to_left,\n",
    "    \"gamma\": st.uniform(0, 10),\n",
    "    'reg_alpha': from_zero_positive,\n",
    "    \"min_child_weight\": from_zero_positive,\n",
    "}\n",
    "\n",
    "xgbreg = XGBRegressor(nthreads=-1,n_estimators=500, cv=5)  \n",
    "gs8 = RandomizedSearchCV(xgbreg, params)  \n",
    "gs8.fit(X_PCA_train, y_train.values.ravel())  \n",
    "gs8.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs8.predict(X_PCA_test)\n",
    "full_predictions = gs8.predict(X_PCA)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The values of this array sum to 1, unless all trees are single node trees consisting of only the root node, \n",
    "#in which case it will be an array of zeros.\n",
    "model = XGBRegressor(nthreads=-1,n_estimators=500,colsample_bytree=0.8385749300087559,gamma=4.711994964749211,\n",
    "                     learning_rate=0.02198803862710416,max_depth=17,min_child_weight=114.35959341213693,\n",
    "                     reg_alpha=59.19676051250868,subsample=0.9732956894126421)\n",
    "model.fit(X_PCA_train, y_train)\n",
    "\n",
    "fold_importance_df = pd.DataFrame()\n",
    "feature_importance_df = pd.DataFrame()\n",
    "fold_importance_df[\"Feature\"] = principalDf.columns\n",
    "fold_importance_df[\"importance\"] = model.feature_importances_[:len(principalDf.columns)]\n",
    "feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:200].index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,26))\n",
    "sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('XGBRegressor Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('XGBRegressor_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "grid = {\n",
    "    'max_depth': st.randint(4,12),\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': st.randint(2,12),\n",
    "    'min_samples_split': st.randint(10,100),\n",
    "}\n",
    "#params = grid_search_cv(DecisionTreeRegressor(criterion='mae'),\n",
    "                        #grid, X_train_scaled, target)\n",
    "#rf_oof = make_predictions(DecisionTreeRegressor(splitter='random', **params),\n",
    "                         # X_train_scaled, target)\n",
    "    \n",
    "rfr = DecisionTreeRegressor(criterion='mae')\n",
    "gs9 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs9.fit(X_PCA_train, y_train.values.ravel())  \n",
    "gs9.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs9.predict(X_PCA_test)\n",
    "full_predictions = gs9.predict(X_PCA)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "grid = {\n",
    "    #'max_depth': [4, 6, 8, 10, 12],\n",
    "    #'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    #'min_samples_leaf': [2, 4, 12],\n",
    "    #'min_samples_split': [4,8,12,16,20, 30, 40, 50, 60],\n",
    "    \"learning_rate\": st.uniform(0.01, 0.4),\n",
    "    'loss' : [\"square\"],\n",
    "    #'n_estimators' : st.randint(10000, 20000)\n",
    "}\n",
    "#params = grid_search_cv(AdaBoostRegressor(),\n",
    "                        #grid, X_train_scaled, target)\n",
    "#rf_oof = make_predictions(AdaBoostRegressor(**params),\n",
    "                          #X_train_scaled, target)\n",
    "base = Ridge(alpha=1) \n",
    "rfr = AdaBoostRegressor(n_estimators = 500, base_estimator=base)\n",
    "gs10 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs10.fit(X_PCA_train, y_train.values.ravel())  \n",
    "gs10.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs10.predict(X_PCA_test)\n",
    "full_predictions = gs10.predict(X_PCA)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import scipy.stats as st\n",
    "\n",
    "grid = {\n",
    "     'learning_rate': [0.1, 0.05, 0.01, 0.005],\n",
    "    'num_leaves': list(range(8, 92, 4)),\n",
    "    'max_depth': [3, 4, 5, 6, 8, 12, 16, -1],\n",
    "    'feature_fraction': [0.8, 0.85, 0.9, 0.95, 1],\n",
    "    'subsample': [0.8, 0.85, 0.9, 0.95, 1],\n",
    "    'lambda_l1': [0, 0.1, 0.2, 0.4, 0.6, 0.9],\n",
    "    'lambda_l2': [0, 0.1, 0.2, 0.4, 0.6, 0.9],\n",
    "    'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'min_gain_to_split': [0, 0.001, 0.01, 0.1],\n",
    "    'random_seed': st.randint(10, 100),\n",
    "    }\n",
    "#params = grid_search_cv(RandomForestRegressor(criterion='mse', n_estimators=50),\n",
    "                        #grid, X_train_scaled, target)\n",
    "rfr = lgb.LGBMRegressor(n_estimators=10000)\n",
    "gs11 = RandomizedSearchCV(rfr, param_distributions = grid)  \n",
    "gs11.fit(X_PCA_train, y_train.values.ravel())  \n",
    "gs11.best_params_    \n",
    "#rf_oof = make_predictions(RandomForestRegressor(**params),\n",
    "                          #X_train_scaled, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs11.predict(X_PCA_test)\n",
    "full_predictions = gs11.predict(X_PCA)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "grid = {\n",
    "    'max_depth': [6,11,15,20],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [10,16,20,40],\n",
    "    'min_samples_split': [50,98,150,200],\n",
    "    'n_estimators': [100, 200, 495,1000],\n",
    "    'min_weight_fraction_leaf':  st.uniform(0, 0.5)\n",
    "    }\n",
    "\n",
    "etr = ExtraTreesRegressor(criterion='mae')\n",
    "gs12 = RandomizedSearchCV(etr, param_distributions = grid)  \n",
    "gs12.fit(X_PCA_train, y_train.values.ravel())  \n",
    "gs12.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  gs12.predict(X_PCA_test)\n",
    "full_predictions = gs12.predict(X_PCA)\n",
    "print('mean_absolute_error_sqrtTime:', mean_absolute_error(y_test, predictions))\n",
    "print('r2_score_sqrtTime:', r2_score(y_test, predictions))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual sqrt train Time')\n",
    "ax1.set_ylabel('predicted sqrt train Time')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('sqrt time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('sqrt time to failure')\n",
    "ax1.scatter(y_test.values, predictions, color='brown')\n",
    "ax1.plot([(0, 0), (5, 5)], [(0, 0), (5, 5)], color='blue')\n",
    "ax2.plot(y_test.values, color='blue')\n",
    "ax2.plot(predictions, color='red')\n",
    "ax3.plot(full_predictions, color='red')\n",
    "ax3.plot(Y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.square(predictions)\n",
    "y_test_nolog = np.square(y_test)\n",
    "y_nolog = np.square(Y)\n",
    "y_nolog_predict = np.square(full_predictions)\n",
    "print('mean_absolute_error:', mean_absolute_error(y_test_nolog, y_predict))\n",
    "print('r2_score:', r2_score(y_test_nolog, y_predict))\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(25,5))\n",
    "ax1, ax2, ax3 = axis\n",
    "ax1.set_xlabel('actual')\n",
    "ax1.set_ylabel('predicted')\n",
    "ax2.set_xlabel('train index')\n",
    "ax2.set_ylabel('time to failure')\n",
    "ax3.set_xlabel('train & test index')\n",
    "ax3.set_ylabel('time to failure')\n",
    "ax1.scatter(y_test_nolog.values, y_predict, color='brown')\n",
    "ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "ax2.plot(y_test_nolog.values, color='blue')\n",
    "ax2.plot(y_predict, color='red')\n",
    "ax3.plot(y_nolog, color='blue')\n",
    "ax3.plot(y_nolog_predict, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "X_test = pd.DataFrame(columns=X.columns, dtype=np.float64, index=submission.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg_id in X_test.index:\n",
    "    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "    \n",
    "    x = seg['acoustic_data']  # pd series\n",
    "            \n",
    "    I_ave10 = 0.1*len(x)\n",
    "    II_ave10 = 0.2*len(x)\n",
    "    III_ave10 = 0.3*len(x)\n",
    "    IV_ave10 = 0.4*len(x)\n",
    "    V_ave10 = 0.5*len(x)\n",
    "    VI_ave10 = 0.6*len(x)\n",
    "    VII_ave10 = 0.7*len(x)\n",
    "    VIII_ave10 = 0.8*len(x)\n",
    "    IX_ave10 = 0.9*len(x)\n",
    "    \n",
    "    X_test.loc[seg_id, 'rFFT mean'] = rfft(x).mean()\n",
    "    X_test.loc[seg_id, 'rFFT std'] = rfft(x).std()\n",
    "    \n",
    "    X_test.loc[seg_id, '0<x<5'] = sum(x.between(0, 5))\n",
    "    X_test.loc[seg_id, '5<x<10'] = sum(x.between(5, 10))    \n",
    "    X_test.loc[seg_id, '10<x<20'] = sum(x.between(10, 20))\n",
    "    X_test.loc[seg_id, '20<x<30'] = sum(x.between(20, 30))\n",
    "    X_test.loc[seg_id, '30<x<100'] = sum(x.between(30, 100))\n",
    "    X_test.loc[seg_id, '100<x<500'] = sum(x.between(100, 500))\n",
    "    X_test.loc[seg_id, '500<x<1000'] = sum(x.between(500, 1000))\n",
    "    X_test.loc[seg_id, '1000<x<5000'] = sum(x.between(1000, 5000))\n",
    "    \n",
    "    X_test.loc[seg_id, 'skewness'] = moment(x, moment=3)\n",
    "    X_test.loc[seg_id, 'kurtosis'] = moment(x, moment=4)\n",
    "    X_test.loc[seg_id, 'hypermoment'] = moment(x, moment=5)\n",
    "    X_test.loc[seg_id, 'hypermoment2'] = moment(x, moment=6)\n",
    "    X_test.loc[seg_id, 'ave'] = x.values.mean()\n",
    "    X_test.loc[seg_id, 'std'] = x.values.std()\n",
    "    X_test.loc[seg_id, 'max'] = x.values.max()\n",
    "    X_test.loc[seg_id, 'min'] = x.values.min()\n",
    "        \n",
    "    X_test.loc[seg_id, 'q90'] = np.quantile(x.values, 0.90)\n",
    "    X_test.loc[seg_id, 'q95'] = np.quantile(x.values, 0.95)\n",
    "    X_test.loc[seg_id, 'q99'] = np.quantile(x.values, 0.99)\n",
    "    X_test.loc[seg_id, 'q05'] = np.quantile(x.values, 0.05)\n",
    "    X_test.loc[seg_id, 'q10'] = np.quantile(x.values, 0.10)\n",
    "    X_test.loc[seg_id, 'q01'] = np.quantile(x.values, 0.01)\n",
    "    \n",
    "    X_test.loc[seg_id, 'abs_max'] = np.abs(x.values).max()\n",
    "    X_test.loc[seg_id, 'abs_mean'] = np.abs(x.values).mean()\n",
    "    X_test.loc[seg_id, 'abs_std'] = np.abs(x.values).std()\n",
    "        \n",
    "     # New features - rolling features\n",
    "    for w in [100, 1000, 10000]:\n",
    "        x_roll_std = x.rolling(w).std().dropna().values\n",
    "        x_roll_mean = x.rolling(w).mean().dropna().values\n",
    "        x_roll_abs_mean = x.abs().rolling(w).mean().dropna().values\n",
    "        \n",
    "        X_test.loc[seg_id, 'ave_roll_std_' + str(w)] = x_roll_std.mean()\n",
    "        X_test.loc[seg_id, 'std_roll_std_' + str(w)] = x_roll_std.std()\n",
    "        X_test.loc[seg_id, 'max_roll_std_' + str(w)] = x_roll_std.max()\n",
    "        X_test.loc[seg_id, 'min_roll_std_' + str(w)] = x_roll_std.min()\n",
    "                \n",
    "        X_test.loc[seg_id, 'ave_roll_mean_' + str(w)] = x_roll_mean.mean()\n",
    "        X_test.loc[seg_id, 'std_roll_mean_' + str(w)] = x_roll_mean.std()\n",
    "        X_test.loc[seg_id, 'max_roll_mean_' + str(w)] = x_roll_mean.max()\n",
    "        X_test.loc[seg_id, 'min_roll_mean_' + str(w)] = x_roll_mean.min()\n",
    "        \n",
    "        X_test.loc[seg_id, 'q01_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.01)\n",
    "        X_test.loc[seg_id, 'q05_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.05)\n",
    "        X_test.loc[seg_id, 'q10_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.10)\n",
    "        X_test.loc[seg_id, 'q95_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.95)\n",
    "        X_test.loc[seg_id, 'q99_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.99)\n",
    "        X_test.loc[seg_id, 'q01_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.01)\n",
    "        X_test.loc[seg_id, 'q05_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.05)\n",
    "        X_test.loc[seg_id, 'q95_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.95)\n",
    "        X_test.loc[seg_id, 'q99_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.99)\n",
    "        X_test.loc[seg_id, 'ave_roll_abs_mean_' + str(w)] = x_roll_abs_mean.mean()\n",
    "        X_test.loc[seg_id, 'std_roll_abs_mean_' + str(w)] = x_roll_abs_mean.std()\n",
    "        X_test.loc[seg_id, 'max_roll_abs_mean_' + str(w)] = x_roll_abs_mean.max()\n",
    "        X_test.loc[seg_id, 'min_roll_abs_mean_' + str(w)] = x_roll_abs_mean.min()\n",
    "        X_test.loc[seg_id, 'q01_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.01)\n",
    "        X_test.loc[seg_id, 'q05_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.05)\n",
    "        X_test.loc[seg_id, 'q95_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.95)\n",
    "        X_test.loc[seg_id, 'q99_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scl_obj.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gs6.predict(X_test_scaled)\n",
    "submission['time_to_failure'] = np.square(predictions)\n",
    "submission[submission < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_ExtraTreesRegressor.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
